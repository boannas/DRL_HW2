{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRA 503: Deep Reinforment Learning\n",
    "HW 2 Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "\n",
    "1. Understand how a reinforcement learning agent learns (i.e., evaluates and improves its policy) in an environment where the true dynamic model is unknown.\n",
    "\n",
    "2. Gain insight into different reinforcement learning algorithms, including Monte Carlo methods, the SARSA algorithm, Q-learning, and Double Q-learning. Analyze their strengths and weaknesses.\n",
    "\n",
    "3. Explore approaches to implementing reinforcement learning in real-world scenarios where the state and action spaces are continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1 : Setting up `Cart-Pole` Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. RL Base class**\n",
    "\n",
    "- Constructor (__init__) to initialize the following parameters:\n",
    "\n",
    "    - Control type: Enumeration of RL algorithms used for decision-making (i.e. Monte Carlo, Temporal Difference, Q-learning, or Double Q-learning).\n",
    "\n",
    "    - Number of actions: The total number of discrete actions available to the agent.\n",
    "\n",
    "    - Action range: The minimum and maximum values defining the range of possible actions.\n",
    "\n",
    "    - Discretize state weight: Weighting factor applied when discretizing the state space for learning.\n",
    "\n",
    "    - Learning rate: Determines how quickly the model updates based on new information.\n",
    "\n",
    "    - Initial epsilon: The starting probability of taking a random action in an ε-greedy policy.\n",
    "\n",
    "    - Epsilon decay rate: The rate at which epsilon decreases over time to favor exploitation over exploration.\n",
    "\n",
    "    - Final epsilon: The lowest value epsilon can reach, ensuring some level of exploration remains.\n",
    "\n",
    "    - Discount factor: A coefficient (γ) that determines the importance of future rewards in decision-making.\n",
    "\n",
    "- Core Functions\n",
    "\n",
    "    - get_discretize_action(): Returns a discrete action based on the current policy.\n",
    "\n",
    "    - mapping_action(): Converts a discrete action back into a continuous action within the defined action range.\n",
    "\n",
    "    - discretize_state(): Discretizes and scales the state based on observation weights.\n",
    "\n",
    "    - decay_epsilon(): Decreases epsilon over time and returns the updated value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Algorithm folder**\n",
    "\n",
    "Implement the algorithm\n",
    "\n",
    "- Monte Carlo class\n",
    "\n",
    "```py\n",
    "    print(\"hello, world\")\n",
    "```\n",
    "\n",
    "- SARSA class\n",
    "\n",
    "```py\n",
    "    print(\"hello, world\")\n",
    "```\n",
    "\n",
    "- Q-Learning Class\n",
    "\n",
    "```py\n",
    "    print(\"hello, world\")\n",
    "```\n",
    "\n",
    "- Double Q-Learning Class\n",
    "\n",
    "```py\n",
    "    print(\"hello, world\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 : Training & Playing to stabilize `Cart-Pole` Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find hyperparameter from tuning single model (Q-learning) and use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Epsilon decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 : Evaluate `Cart-Pole` Agent performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asdasdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
